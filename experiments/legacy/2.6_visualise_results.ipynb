{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '/Users/tania/Documents/GitHub/critical-learning-period-effect/artifacts/final_results/MNIST_variations'\n",
    "csv_files = [f for f in os.listdir(results_dir) if f.endswith('.csv')]\n",
    "print(csv_files)\n",
    "\n",
    "results = [\n",
    "    ('Base model - Multi-channel MNIST', '0_MNIST_variations_multi-channel-example_results.csv'),\n",
    "    ('MNIST Colour (source=1 target=0.999)', '1_colourMNIST_source1_target0.999_target0_oct_results.csv'),\n",
    "    ('Noisy MNIST Colour (source=1 target=0.999)', '2_colourMNIST_source1_target0.999_target0_botleneck_results.csv'),\n",
    "    ('Noisy MNIST Colour (source=1 target=0.999)', '3_colourMNISTnoisy_source1_target0.999_eval0_colourNoiseSTD0.07_results.csv'),\n",
    "    ('Noisy MNIST Colour (source=1 target=0.999)', \"4_colourMNISTnoisy_source1_target0.999_eval0_colourNoiseSTD0.07__class 'utils.models.mlp.BasicClassifierModule'__results.csv\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "experiment_name = results[i][0]\n",
    "fp = results[i][1]\n",
    "results_df = pd.read_csv(os.path.join(results_dir, fp))\n",
    "runs = 1\n",
    "\n",
    "print(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['initialisation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_curves(\n",
    "    results_df: pd.DataFrame,\n",
    "    target_eval: str = \"MNIST_hard_target\",\n",
    "    source_keywords: List[str] = [\"pretraining\"],\n",
    "    accuracy=True\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot learning curves (mean ± SEM) for different initialisations on a chosen evaluation set.\n",
    "    Produces two plots: one for non-source initialisations and one for source/pretraining initialisations.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame containing run histories and initial evaluation columns.\n",
    "        target_eval: Evaluation set prefix, e.g. \"MNIST_hard_target\".\n",
    "        source_keywords: Keywords used to identify pretraining/source initialisations.\n",
    "    \"\"\"\n",
    "    if accuracy:\n",
    "        acc_col = f\"{target_eval}_acc\"\n",
    "        init_acc_col = f\"initial_{target_eval}_logits_acc\"\n",
    "        label = \"Accuracy\"\n",
    "    else:\n",
    "        acc_col = f\"{target_eval}_loss\"\n",
    "        init_acc_col = f\"initial_{target_eval}_logits_loss\"\n",
    "        label = \"Loss\"\n",
    "\n",
    "    if acc_col not in results_df.columns or init_acc_col not in results_df.columns:\n",
    "        raise ValueError(f\"Required columns not found for {target_eval}: {acc_col}, {init_acc_col}\")\n",
    "\n",
    "    # Split initialisations into two groups\n",
    "    all_inits = results_df['initialisation'].unique()\n",
    "    pre_training = [init for init in all_inits if any(k.lower() in init.lower() for k in source_keywords)]\n",
    "    target_models = [init for init in all_inits if init not in pre_training]\n",
    "\n",
    "    def make_plot(sub_df, inits_to_plot, title_suffix):\n",
    "        # Aggregate over epochs\n",
    "        agg_acc = (\n",
    "            sub_df[sub_df['initialisation'].isin(inits_to_plot)]\n",
    "            .groupby(['initialisation', 'epoch'])\n",
    "            .agg(acc_mean=(acc_col, 'mean'), acc_sem=(acc_col, 'sem'))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Initial accuracy row (epoch = 0)\n",
    "        per_run_init = (\n",
    "            sub_df[sub_df['initialisation'].isin(inits_to_plot)]\n",
    "            .groupby(['initialisation', 'run'])[init_acc_col]\n",
    "            .first()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        initial_rows = (\n",
    "            per_run_init\n",
    "            .groupby('initialisation')[init_acc_col]\n",
    "            .agg(acc_mean='mean', acc_sem='sem')\n",
    "            .reset_index()\n",
    "            .assign(epoch=0)\n",
    "        )\n",
    "\n",
    "        # Combine and sort\n",
    "        agg_acc = pd.concat([initial_rows, agg_acc], ignore_index=True)\n",
    "        agg_acc = agg_acc.sort_values(['initialisation', 'epoch']).reset_index(drop=True)\n",
    "\n",
    "        # Plot\n",
    "        fig = go.Figure()\n",
    "        colors = px.colors.sample_colorscale('Viridis', np.linspace(0, 1, len(inits_to_plot)))\n",
    "        color_map = dict(zip(inits_to_plot, colors))\n",
    "\n",
    "        for init in inits_to_plot:\n",
    "            df_init = agg_acc[agg_acc['initialisation'] == init]\n",
    "            color = color_map[init]\n",
    "\n",
    "            # Mean curve\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_init['epoch'],\n",
    "                y=df_init['acc_mean'],\n",
    "                mode='lines',\n",
    "                name=f\"{init}<br>(final {df_init['acc_mean'].iloc[-1]:.3f} ± {df_init['acc_sem'].iloc[-1]:.3f})\",\n",
    "                line=dict(color=color, width=2)\n",
    "            ))\n",
    "\n",
    "            # SEM shading\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df_init['epoch'].tolist() + df_init['epoch'][::-1].tolist(),\n",
    "                y=(df_init['acc_mean'] + df_init['acc_sem']).tolist() +\n",
    "                  (df_init['acc_mean'] - df_init['acc_sem'])[::-1].tolist(),\n",
    "                fill='toself',\n",
    "                fillcolor=color.replace('rgb', 'rgba').replace(')', ',0.15)'),\n",
    "                line=dict(color='rgba(255,255,255,0)'),\n",
    "                hoverinfo=\"skip\",\n",
    "                showlegend=False,\n",
    "            ))\n",
    "\n",
    "            # Individual runs\n",
    "            for source_file, df_run in sub_df[sub_df['initialisation'] == init].groupby('source'):\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=df_run['epoch'],\n",
    "                    y=df_run[acc_col],\n",
    "                    mode='lines',\n",
    "                    line=dict(color=color, width=1),\n",
    "                    opacity=0.25,\n",
    "                    name=f'{init} - {source_file}',\n",
    "                    showlegend=False\n",
    "                ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{i}: Performance on {experiment_name} {title_suffix} (mean ± SEM)\",\n",
    "            xaxis_title=\"Epoch\",\n",
    "            yaxis_title=f'{label} on {target_eval}',\n",
    "            template=\"plotly_white\",\n",
    "            legend_title=\"Initialisation\",\n",
    "        )\n",
    "        fig.show()\n",
    "    # Plot nonsource initialisations\n",
    "    if target_models:\n",
    "        make_plot(results_df, target_models, \"\")\n",
    "\n",
    "    # Plot source/pretraining initialisations\n",
    "    if pre_training:\n",
    "        make_plot(results_df, pre_training, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_single_initialisation_runs(\n",
    "    results_df: pd.DataFrame,\n",
    "    init_name: str,\n",
    "    target_eval: str = \"MNIST_hard_target\",\n",
    "    accuracy=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot all runs for a single initialisation on a given evaluation metric.\n",
    "    Draws individual run curves as opaque lines, and shows initial epoch-0 points.\n",
    "    No mean curve or shading.\n",
    "    \"\"\"\n",
    "    if accuracy:\n",
    "        acc_col = f\"{target_eval}_acc\"\n",
    "        init_acc_col = f\"initial_{target_eval}_logits_acc\"\n",
    "        label = \"Accuracy\"\n",
    "    else:\n",
    "        acc_col = f\"{target_eval}_loss\"\n",
    "        init_acc_col = f\"initial_{target_eval}_logits_loss\"\n",
    "        label = \"Loss\"\n",
    "    \n",
    "    if acc_col not in results_df.columns or init_acc_col not in results_df.columns:\n",
    "        raise ValueError(f\"Missing required columns for {target_eval}: {acc_col}, {init_acc_col}\")\n",
    "\n",
    "    df_init = results_df[results_df['initialisation'] == init_name]\n",
    "    if df_init.empty:\n",
    "        raise ValueError(f\"No runs found for initialisation '{init_name}'\")\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot each run line and its initial point\n",
    "    for source_file, df_run in df_init.groupby('source'):\n",
    "        init_val = df_run[init_acc_col].iloc[0]\n",
    "\n",
    "        # Run curve\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[0] + df_run['epoch'].tolist(),\n",
    "            y=[init_val] + df_run[acc_col].tolist(),\n",
    "            mode='lines',\n",
    "            name=str(source_file),\n",
    "            line=dict(width=2),\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"All runs for '{init_name}' on {target_eval}\",\n",
    "        xaxis_title='Epoch',\n",
    "        yaxis_title=label,\n",
    "        template='plotly_white',\n",
    "        legend_title='Run',\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = True # Plot accuracy or loss\n",
    "\n",
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_hard_target\", accuracy=accuracy)\n",
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_test_target\", accuracy=accuracy)\n",
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_hard_eval\", accuracy=accuracy)\n",
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_hard_gray\", accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_hard_target\", accuracy=accuracy)\n",
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_test_target\", accuracy=accuracy)\n",
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_hard_eval\", accuracy=accuracy)\n",
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_hard_gray\", accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_hard_target\")\n",
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_test_target\")\n",
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_hard_eval\")\n",
    "plot_accuracy_curves(results_df, target_eval=\"MNIST_hard_gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = False\n",
    "metric = \"MNIST_test_target\"\n",
    "plot_single_initialisation_runs(results_df, \"target with random init\", target_eval=metric, accuracy=accuracy)\n",
    "plot_single_initialisation_runs(results_df, \"pretraining on source from random init\", target_eval=metric, accuracy=accuracy)\n",
    "plot_single_initialisation_runs(results_df, \"target with source pre-trained model init\", target_eval=metric, accuracy=accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "critical-learning-period-effect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
